{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HxyScotthuang/mwp-cl/blob/main/fit_CL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xbVRXPv_SfK",
        "outputId": "41c332a7-ca21-4d1a-f472-4aea9377da21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQvPJKDAZcuD",
        "outputId": "52d92c74-3c35-4349-9960-3dc5bc428ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install SentencePiece\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dJo_3iBI_Q7b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/bert+ch/UniLM4MWP')\n",
        "#os.chdir('/content/drive/MyDrive/bert+ch/UniLM4MWP')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S5m6WPV_5u1"
      },
      "source": [
        "Here we try to finetune the CL-data by loading the value of our CL pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IgstrTlrA8AE"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModel,BertGenerationEncoder,BertGenerationDecoder,EncoderDecoderModel,BertTokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJOS_2RzEj4G",
        "outputId": "a08a1262-8b22-4c3b-ba3e-da759624fe73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21128, 768])\n"
          ]
        }
      ],
      "source": [
        "CL_model = AutoModel.from_pretrained(\"Math23k_CL\")\n",
        "embedding_matrix = CL_model.embeddings.word_embeddings.weight\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"Math23k_CL\")\n",
        "print(np.shape(embedding_matrix))\n",
        "#torch.Size([30522, 768])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQBOaw_gFNZ1"
      },
      "source": [
        "Now we have extracted the word embedding for our given vocabulary. So we can just focus on the finetune part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kdszvjdZGrty"
      },
      "outputs": [],
      "source": [
        "def is_equal(a, b):\n",
        "    \"\"\"考虑多次浮点计算会出现无法取整的情况，\n",
        "       用round()对两个结果取整检查是否相等\n",
        "    \"\"\"\n",
        "    a = round(float(a), 6)\n",
        "    b = round(float(b), 6)\n",
        "    return a == b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HzhalaNgF88q"
      },
      "outputs": [],
      "source": [
        "def remove_bucket(equation):\n",
        "    l_buckets, buckets = [], []\n",
        "    for i, c in enumerate(equation):\n",
        "        if c == '(':\n",
        "            l_buckets.append(i)\n",
        "        elif c == ')':\n",
        "            buckets.append((l_buckets.pop(), i))\n",
        "    try:\n",
        "      eval_equation = eval(equation)\n",
        "    except:\n",
        "      pass\n",
        "      #print(equation)\n",
        "      \n",
        "    for l, r in buckets:\n",
        "        new_equation = '%s %s %s' % (\n",
        "            equation[:l], equation[l + 1:r], equation[r + 1:]\n",
        "        )\n",
        "        try:\n",
        "            if is_equal(eval(new_equation.replace(' ', '')), eval_equation):\n",
        "                equation = new_equation\n",
        "        except:\n",
        "            pass\n",
        "    return equation.replace(' ', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "81vETGsCGPzj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def preprocess (question, equation, answer): \n",
        "    # 处理 中括号\n",
        "    equation = equation.replace('[', '(').replace(']', ')').replace('^','**')\n",
        "    # \\d+:表达一个或多个数字；\\1表示向前引用\n",
        "    # 例如1(2/3)先替换为(1+2/3)，再替换为1+(2/3)\n",
        "    # 处理带分数\n",
        "    question = re.sub('(\\d+)\\((\\d+/\\d+)\\)', '(\\\\1+\\\\2)', question)\n",
        "    equation = re.sub('(\\d+)\\((\\d+/\\d+)\\)', '(\\\\1+\\\\2)', equation)\n",
        "    answer = re.sub('(\\d+)\\((\\d+/\\d+)\\)', '(\\\\1+\\\\2)', answer)\n",
        "    equation = re.sub('(\\d+)\\(', '\\\\1+(', equation)\n",
        "    answer = re.sub('(\\d+)\\(', '\\\\1+(', answer)\n",
        "    # 分数去括号\n",
        "    question = re.sub('\\((\\d+/\\d+)\\)', '\\\\1', question)\n",
        "    # 处理百分数\n",
        "    equation = re.sub('([\\.\\d]+)%', '(\\\\1/100)', equation)\n",
        "    answer = re.sub('([\\.\\d]+)%', '(\\\\1/100)', answer)\n",
        "    # 冒号转除号、剩余百分号处理\n",
        "    equation = equation.replace(':', '/').replace('%', '/100')\n",
        "    answer = answer.replace(':', '/').replace('%', '/100')\n",
        "    if equation[:2] == 'x=':\n",
        "        equation = equation[2:]\n",
        "    \n",
        "    return question, remove_bucket(equation), answer\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9fUUymjDeGwg"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"Math23k_CL\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wwPN0c7q0Alf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "class MyDataSet(Dataset):\n",
        "  def __init__(self, json_path,tokenizer,start = 0):\n",
        "    self.json_path = json_path\n",
        "    # MathQA_train.json\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = json.load(open(self.json_path, 'r'))[start:]\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    # This method should return only 1 sample and label \n",
        "    # (according to \"index\"), not the whole dataset\n",
        "    # So probably something like this for you:\n",
        "    question = self.data[index][\"segmented_text\"]\n",
        "    \n",
        "    label = self.data[index][\"equation\"]\n",
        "    ans = self.data[index][\"ans\"]\n",
        "\n",
        "    q,eq,ans = preprocess(question, label, ans)\n",
        "    inputs = self.tokenizer(q,padding=\"max_length\",\n",
        "            max_length=64,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\")\n",
        "    inputs_id = inputs.input_ids[0]\n",
        "    attention_mask = inputs.attention_mask[0]\n",
        "    label = self.tokenizer(eq, padding=\"max_length\",\n",
        "          max_length=64,\n",
        "          truncation=True,\n",
        "          add_special_tokens=True,\n",
        "          return_tensors=\"pt\").input_ids[0]\n",
        "    label = torch.tensor([-100 if item == tokenizer.pad_token_id else item for item in label])\n",
        "    outs = {\"input_ids\":inputs_id,\"label\":label,\"attention_mask\":attention_mask,\"text\":q,\"eq\":eq,\"ans\":ans}\n",
        "    return outs\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2iNm8Phz2jr2"
      },
      "outputs": [],
      "source": [
        "# loading the data\n",
        "#train_data = MyDataSet('dataset/math23k/trainset.json')\n",
        "#train_data = MyDataSet('dataset/math23k/trainset.json',tokenizer,start = 5000)\n",
        "train_data = MyDataSet('dataset/math23k/trainset.json',tokenizer)\n",
        "\n",
        "test_data = MyDataSet('dataset/math23k/testset.json',tokenizer)\n",
        "val_data = MyDataSet('dataset/math23k/validset.json',tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=8\n",
        " )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=8\n",
        " )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_data,\n",
        "    batch_size=8\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1]"
      ],
      "metadata": {
        "id": "4ycSbMWeoTp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff02e470-6301-42f0-f46b-411b69e312a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 101,  671, 7555, 2339, 4923, 8024, 4508,  510,  734,  697, 7339, 1394,\n",
              "          976, 8114, 1921, 2130, 2768, 8024, 4385, 1762, 4508, 7339, 1296, 4324,\n",
              "          976, 8125, 1921, 1400, 8024,  734, 7339, 1217, 1057, 8024,  697, 7339,\n",
              "         1348, 1394,  976,  749, 8110, 1921, 8024, 6821, 3198, 4508, 7339, 6444,\n",
              "         6624, 8024,  734, 7339, 5326, 5330,  976, 8115, 1921, 2798, 2130, 2768,\n",
              "         6821, 7555, 2339,  102]),\n",
              " 'label': tensor([ 101,  122,  120,  113,  113,  122,  118,  113, 8110,  116, 8115,  114,\n",
              "          120, 8114,  114,  120,  113, 8125,  118, 8115,  114,  114,  102, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'text': '一 项 工程 ， 甲 、 乙 两队 合 做 30 天 完成 ， 现在 甲队 单独 做 24 天 后 ， 乙 队 加入 ， 两队 又 合 做 了 12 天 ， 这时 甲队 调 走 ， 乙 队 继续 做 15 天才 完成 这项 工程 ． 甲队 单独 做 这项 工程 需 多少 天 ？',\n",
              " 'eq': '1/((1-(12+15)/30)/(24-15))',\n",
              " 'ans': '90'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ystLI5ADzJxr"
      },
      "outputs": [],
      "source": [
        "class My_model(torch.nn.Module):\n",
        "  def __init__(self,bert,embedding_matrix):\n",
        "    super(My_model, self).__init__()\n",
        "    self.embedding = torch.nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).float())\n",
        "    self.embedding.weight.requires_grad = False\n",
        "    self.bert_model = bert\n",
        "  def forward(self,input_ids,decoder_input_ids,labels = None,return_dict=True):\n",
        "    inputs_embeds = self.embedding(inputs_ids)\n",
        "    outputs = self.bert_model(inputs_embeds=inputs_embeds,decoder_input_ids=decoder_input_ids, labels=labels,return_dict = return_dict)\n",
        "    return outputs\n",
        "  def generate(self,inputs_id):\n",
        "    inputs_embeds = self.embedding(inputs_ids)\n",
        "    return self.bert_model.generate(inputs_embeds = inputs_embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NbVCMSYeoRyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1fbe31d-1bf1-40ab-a521-0044d6c69ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertGenerationEncoder: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertGenerationEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertGenerationEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertGenerationDecoder: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertGenerationDecoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertGenerationDecoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertGenerationDecoder were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'lm_head.decoder.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'lm_head.decoder.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'lm_head.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "encoder = BertGenerationEncoder.from_pretrained(\"bert-base-chinese\", bos_token_id=101, eos_token_id=102)\n",
        "# add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n",
        "decoder = BertGenerationDecoder.from_pretrained(\n",
        "    \"bert-base-chinese\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n",
        ")\n",
        "bert_model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
        "model = bert_model\n",
        "#model = My_model(bert_model,embedding_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ssCJL09GG6wK"
      },
      "outputs": [],
      "source": [
        "ContinueTrain = False\n",
        "if ContinueTrain:\n",
        "  out_path = \"finetune_out/bert_model_baseline\"\n",
        "\n",
        "  model = torch.load(out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3azn0FbBIdcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19d782c-7cbe-40a3-9199-ee69f02e528e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoderModel(\n",
              "  (encoder): BertGenerationEncoder(\n",
              "    (embeddings): BertGenerationEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): BertGenerationDecoder(\n",
              "    (bert): BertGenerationEncoder(\n",
              "      (embeddings): BertGenerationEmbeddings(\n",
              "        (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (lm_head): BertGenerationOnlyLMHead(\n",
              "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "optimizer = Adam(model.parameters(),lr=2e-5)\n",
        "out_path = \"/finetune_out/\"\n",
        "num_epoch = 50\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "model.config.max_length = 500\n",
        "model.config.min_length = 1\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.early_stopping = True\n",
        "model.config.length_penalty = 1.0\n",
        "model.config.num_beams = 5\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "%%capture\n",
        "!pip install git-python==1.0.3\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p1PkoSCODz8U",
        "outputId": "160be689-fb8b-45bd-99ab-3968651119e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n%%capture\\n!pip install git-python==1.0.3\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "Argument = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    output_dir = \"finetune_out/bert_model_baseline_test\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps = 500,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps = 500,\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 100000,\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size= 8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs= 70,\n",
        "    load_best_model_at_end= True\n",
        ")\n"
      ],
      "metadata": {
        "id": "yFEjnUfOjw2A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model= model,\n",
        "    # The training arguments.\n",
        "    args=Argument,\n",
        "    # The training dataset.\n",
        "    train_dataset=train_data,\n",
        "   \n",
        "    eval_dataset=val_data,\n",
        "    \n",
        "    tokenizer=tokenizer\n",
        "    \n",
        ")\n"
      ],
      "metadata": {
        "id": "OmmcblHmmBCx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Fr-z3VejoENx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bf38643-4d9f-4f93-dbc8-667ece523a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 18529\n",
            "  Num Epochs = 70\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 162190\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='41922' max='162190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 41922/162190 2:49:42 < 8:06:53, 4.12 it/s, Epoch 18.09/70]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.120300</td>\n",
              "      <td>0.248014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.260200</td>\n",
              "      <td>0.193364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.206400</td>\n",
              "      <td>0.170033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.184600</td>\n",
              "      <td>0.166626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.177100</td>\n",
              "      <td>0.163024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>0.165784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.162300</td>\n",
              "      <td>0.156557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.160500</td>\n",
              "      <td>0.148463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.157900</td>\n",
              "      <td>0.152159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.157100</td>\n",
              "      <td>0.156872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.155500</td>\n",
              "      <td>0.151110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.153200</td>\n",
              "      <td>0.154309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.151200</td>\n",
              "      <td>0.143876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.153900</td>\n",
              "      <td>0.150400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.149300</td>\n",
              "      <td>0.151834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.150700</td>\n",
              "      <td>0.148491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.151500</td>\n",
              "      <td>0.144860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.148800</td>\n",
              "      <td>0.150362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.148200</td>\n",
              "      <td>0.149744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.145900</td>\n",
              "      <td>0.149941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.146100</td>\n",
              "      <td>0.149881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.155001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.149600</td>\n",
              "      <td>0.149043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.146200</td>\n",
              "      <td>0.150146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.145300</td>\n",
              "      <td>0.152433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.146600</td>\n",
              "      <td>0.153714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.145000</td>\n",
              "      <td>0.147434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.142900</td>\n",
              "      <td>0.147120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.144700</td>\n",
              "      <td>0.148837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.142700</td>\n",
              "      <td>0.142625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.144700</td>\n",
              "      <td>0.148665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.144300</td>\n",
              "      <td>0.144597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.142400</td>\n",
              "      <td>0.146273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.143700</td>\n",
              "      <td>0.144735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.142700</td>\n",
              "      <td>0.147688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.142800</td>\n",
              "      <td>0.144228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.141400</td>\n",
              "      <td>0.145339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.142700</td>\n",
              "      <td>0.156518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.143800</td>\n",
              "      <td>0.148979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.144908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.142600</td>\n",
              "      <td>0.147800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.151304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.143300</td>\n",
              "      <td>0.147061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.142600</td>\n",
              "      <td>0.142671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.139900</td>\n",
              "      <td>0.147233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.141600</td>\n",
              "      <td>0.145300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.141500</td>\n",
              "      <td>0.144319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.140300</td>\n",
              "      <td>0.146830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.143917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.142400</td>\n",
              "      <td>0.145150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.140600</td>\n",
              "      <td>0.143779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.142000</td>\n",
              "      <td>0.145415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.141000</td>\n",
              "      <td>0.143529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.141500</td>\n",
              "      <td>0.145393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.145018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.142576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.139700</td>\n",
              "      <td>0.143379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.147387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.141400</td>\n",
              "      <td>0.142006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.143250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.139100</td>\n",
              "      <td>0.143336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.143971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.140400</td>\n",
              "      <td>0.144641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.140300</td>\n",
              "      <td>0.144435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.139100</td>\n",
              "      <td>0.145585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.138700</td>\n",
              "      <td>0.143532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.139900</td>\n",
              "      <td>0.144161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.140300</td>\n",
              "      <td>0.146301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.144835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.136800</td>\n",
              "      <td>0.142659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.145217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.139800</td>\n",
              "      <td>0.145604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.144800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.138300</td>\n",
              "      <td>0.146144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.140400</td>\n",
              "      <td>0.143492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.143772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.137600</td>\n",
              "      <td>0.144309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.140100</td>\n",
              "      <td>0.144583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.137700</td>\n",
              "      <td>0.142581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>0.137400</td>\n",
              "      <td>0.144180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>0.140100</td>\n",
              "      <td>0.143240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>0.138900</td>\n",
              "      <td>0.143247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='56284' max='162190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 56284/162190 3:47:44 < 7:08:32, 4.12 it/s, Epoch 24.29/70]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.120300</td>\n",
              "      <td>0.248014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.260200</td>\n",
              "      <td>0.193364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.206400</td>\n",
              "      <td>0.170033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.184600</td>\n",
              "      <td>0.166626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.177100</td>\n",
              "      <td>0.163024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>0.165784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.162300</td>\n",
              "      <td>0.156557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.160500</td>\n",
              "      <td>0.148463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.157900</td>\n",
              "      <td>0.152159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.157100</td>\n",
              "      <td>0.156872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.155500</td>\n",
              "      <td>0.151110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.153200</td>\n",
              "      <td>0.154309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.151200</td>\n",
              "      <td>0.143876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.153900</td>\n",
              "      <td>0.150400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.149300</td>\n",
              "      <td>0.151834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.150700</td>\n",
              "      <td>0.148491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.151500</td>\n",
              "      <td>0.144860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.148800</td>\n",
              "      <td>0.150362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.148200</td>\n",
              "      <td>0.149744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.145900</td>\n",
              "      <td>0.149941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.146100</td>\n",
              "      <td>0.149881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.155001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.149600</td>\n",
              "      <td>0.149043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.146200</td>\n",
              "      <td>0.150146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.145300</td>\n",
              "      <td>0.152433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.146600</td>\n",
              "      <td>0.153714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.145000</td>\n",
              "      <td>0.147434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.142900</td>\n",
              "      <td>0.147120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.144700</td>\n",
              "      <td>0.148837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.142700</td>\n",
              "      <td>0.142625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.144700</td>\n",
              "      <td>0.148665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.144300</td>\n",
              "      <td>0.144597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.142400</td>\n",
              "      <td>0.146273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.143700</td>\n",
              "      <td>0.144735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.142700</td>\n",
              "      <td>0.147688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.142800</td>\n",
              "      <td>0.144228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.141400</td>\n",
              "      <td>0.145339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.142700</td>\n",
              "      <td>0.156518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.143800</td>\n",
              "      <td>0.148979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.144908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.142600</td>\n",
              "      <td>0.147800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.151304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.143300</td>\n",
              "      <td>0.147061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.142600</td>\n",
              "      <td>0.142671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.139900</td>\n",
              "      <td>0.147233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.141600</td>\n",
              "      <td>0.145300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.141500</td>\n",
              "      <td>0.144319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.140300</td>\n",
              "      <td>0.146830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.143917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.142400</td>\n",
              "      <td>0.145150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.140600</td>\n",
              "      <td>0.143779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.142000</td>\n",
              "      <td>0.145415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.141000</td>\n",
              "      <td>0.143529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.141500</td>\n",
              "      <td>0.145393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.145018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.142576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.139700</td>\n",
              "      <td>0.143379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.147387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.141400</td>\n",
              "      <td>0.142006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.143250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.139100</td>\n",
              "      <td>0.143336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.143971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.140400</td>\n",
              "      <td>0.144641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.140300</td>\n",
              "      <td>0.144435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.139100</td>\n",
              "      <td>0.145585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.138700</td>\n",
              "      <td>0.143532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.139900</td>\n",
              "      <td>0.144161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.140300</td>\n",
              "      <td>0.146301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.144835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.136800</td>\n",
              "      <td>0.142659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.145217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.139800</td>\n",
              "      <td>0.145604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.144800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.138300</td>\n",
              "      <td>0.146144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.140400</td>\n",
              "      <td>0.143492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.143772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.137600</td>\n",
              "      <td>0.144309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.140100</td>\n",
              "      <td>0.144583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.137700</td>\n",
              "      <td>0.142581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>0.137400</td>\n",
              "      <td>0.144180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>0.140100</td>\n",
              "      <td>0.143240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>0.138900</td>\n",
              "      <td>0.143247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>0.138400</td>\n",
              "      <td>0.144495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42500</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.145906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.144521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43500</td>\n",
              "      <td>0.137700</td>\n",
              "      <td>0.145305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>0.139500</td>\n",
              "      <td>0.143911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44500</td>\n",
              "      <td>0.138100</td>\n",
              "      <td>0.143707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>0.138200</td>\n",
              "      <td>0.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45500</td>\n",
              "      <td>0.139000</td>\n",
              "      <td>0.143661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>0.138400</td>\n",
              "      <td>0.142372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46500</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.144127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>0.138800</td>\n",
              "      <td>0.144682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47500</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.143634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>0.137300</td>\n",
              "      <td>0.144420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48500</td>\n",
              "      <td>0.136700</td>\n",
              "      <td>0.145033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>0.136900</td>\n",
              "      <td>0.145689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49500</td>\n",
              "      <td>0.137600</td>\n",
              "      <td>0.147763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>0.137600</td>\n",
              "      <td>0.143370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50500</td>\n",
              "      <td>0.137600</td>\n",
              "      <td>0.144553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>0.138300</td>\n",
              "      <td>0.145861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51500</td>\n",
              "      <td>0.138100</td>\n",
              "      <td>0.143874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>0.137700</td>\n",
              "      <td>0.144679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52500</td>\n",
              "      <td>0.137400</td>\n",
              "      <td>0.142941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>0.136800</td>\n",
              "      <td>0.143657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53500</td>\n",
              "      <td>0.137400</td>\n",
              "      <td>0.143457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>0.136800</td>\n",
              "      <td>0.142363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.142387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>0.136400</td>\n",
              "      <td>0.143126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55500</td>\n",
              "      <td>0.137900</td>\n",
              "      <td>0.142767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>0.137700</td>\n",
              "      <td>0.143770</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2316\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text, ans, eq. If text, ans, eq are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "SMAgdPdfKsYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "    print(\"Start epoch \",epoch+1)\n",
        "    running_loss = 0.0\n",
        "    for k, data in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs_ids = data[\"input_ids\"].to(device)\n",
        "        attention_mask = data[\"attention_mask\"].to(device)\n",
        "        labels = data[\"label\"].to(device)\n",
        "        ans = data[\"ans\"]\n",
        "        \n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, label, _,_, ans= data\n",
        "\n",
        "        \n",
        "        # turn tuple to list\n",
        "        inputs = list(inputs)\n",
        "        labels = list(label)\n",
        "        ans = list(ans)\n",
        "        \n",
        "        # tokenize the input\n",
        "        #inputs_ids = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
        "        #labels = tokenizer(labels, return_tensors=\"pt\", padding=True, truncation=True).input_ids  \n",
        "        #inputs_ids = inputs_ids.to(device)\n",
        "       # labels = label.to(device)\n",
        "\n",
        "\n",
        "        # decoder_input_ids = tokenizer(labels[:-1], return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
        "        # labels = tokenizer(labels[1:], return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
        "            \n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs_id=inputs_ids, decoder_inputs_id=labels)\n",
        "        loss = outputs.loss\n",
        "        \n",
        "        outputs_check = model.generate(input_ids = inputs_ids,attention_mask = attention_mask)\n",
        "        for i in range(len(outputs_check)):\n",
        "            pred_eq = tokenizer.decode(outputs_check[i], skip_special_tokens=True)\n",
        "            print(pred_eq)\n",
        "            sum += 1\n",
        "            try:\n",
        "                if is_equal(eval(pred_eq),eval(ans[i])):\n",
        "                  correct += 1\n",
        "            except:\n",
        "              pass\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    print(f'[{epoch + 1}, {k + 1:5d}] loss: {running_loss / len(train_dataloader):.6f}')\n",
        "    #print(\"Accuracy:\"+ str(correct / sum))\n",
        "    running_loss = 0.0\n",
        "'''\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "iTAvpdzdjvo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSM3Olu1Rt-O"
      },
      "outputs": [],
      "source": [
        "out_path = \"finetune_out/bert_model_baseline\"\n",
        "torch.save(model, out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Ftl6hJbt4B"
      },
      "source": [
        "After saving the model, we are free to get out the model here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o82tpPhBRxNF"
      },
      "outputs": [],
      "source": [
        "out_path = \"finetune_out/bert_model_baseline\"\n",
        "\n",
        "model = torch.load(out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1csrNIQQDw_s"
      },
      "outputs": [],
      "source": [
        "# inference\n",
        "import torch.nn.functional as F  #for softmax function \n",
        "model = model.cuda()\n",
        "input = tokenizer(\n",
        "    \"我 有 3 个 电脑，你 有 2 个 电脑，我们 一共 有 多少 个 电脑 ?\", return_tensors=\"pt\", padding=True, truncation=True\n",
        ")# Batch size 1\n",
        "\n",
        "inputs_ids = input.input_ids.to(device)\n",
        "attention_mask = input.attention_mask.to(device)\n",
        "outputs = model.generate(inputs_ids,attention_mask = attention_mask)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwibaxIxZe2S"
      },
      "outputs": [],
      "source": [
        "#inference in batch\n",
        "sum = 0\n",
        "correct = 0\n",
        "for k, data in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "\n",
        "    inputs_ids = data[\"input_ids\"].to(device)\n",
        "    attention_mask = data[\"attention_mask\"].to(device)\n",
        "    labels = data[\"label\"].to(device)\n",
        "    ans = data[\"ans\"]\n",
        "\n",
        "    '''\n",
        "    # turn tuple to list\n",
        "    inputs = list(inputs)\n",
        "    label = list(labels)\n",
        "    ans = list(ans)\n",
        "\n",
        "    # tokenize the input\n",
        "    input = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs_ids = input.input_ids.to(device)\n",
        "    '''\n",
        "    #decoder_input = torch.tensor([[-100]]).to(device)\n",
        "    #outputs = model(input_ids = inputs_ids, decoder_input_ids = decoder_input, return_dict=True)\n",
        "    #outputs = outputs.logits.argmax(-1)\n",
        "    outputs = model.generate(inputs_ids)\n",
        "    for i in range(len(outputs)):\n",
        "        pred_eq = tokenizer.decode(outputs[i], skip_special_tokens=True).replace(\" \",\"\")\n",
        "        print(pred_eq)\n",
        "        sum += 1\n",
        "        try:\n",
        "            if is_equal(eval(pred_eq),eval(ans[i])):\n",
        "              correct += 1\n",
        "        except:\n",
        "            pass\n",
        "          \n",
        "print(\"Accuracy:\"+ str(correct / sum))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline log\n",
        "\n",
        "\n",
        "Start epoch  65\n",
        "100%|██████████| 1692/1692 [03:51<00:00,  7.32it/s]\n",
        "[65,  1692] loss: 0.015328\n",
        "Accuracy:0.7974720969768645\n",
        "<br>\n",
        "Start epoch  66\n",
        "100%|██████████| 1692/1692 [03:54<00:00,  7.23it/s]\n",
        "[66,  1692] loss: 0.008195<br>\n",
        "Accuracy:0.8064158474388351\n",
        "Start epoch  67\n",
        "100%|██████████| 1692/1692 [03:49<00:00,  7.37it/s]\n",
        "[67,  1692] loss: 0.006870<br>\n",
        "Accuracy:0.8098898662133195\n",
        "Start epoch  68\n",
        "100%|██████████| 1692/1692 [04:04<00:00,  6.91it/s]\n",
        "[68,  1692] loss: 0.018374<br>\n",
        "Accuracy:0.7922980264616749\n",
        "Start epoch  69\n",
        "100%|██████████| 1692/1692 [03:53<00:00,  7.26it/s]\n",
        "[69,  1692] loss: 0.007198<br>\n",
        "Accuracy:0.8083376450587627\n",
        "Start epoch  70\n",
        "100%|██████████| 1692/1692 [03:51<00:00,  7.31it/s][70,  1692] loss: 0.009569\n",
        "Accuracy:0.8074506615418731<br>\n",
        "Finished Training\n",
        "\n",
        "\n",
        "\n",
        "100%|██████████| 2317/2317 [01:36<00:00, 24.01it/s]Accuracy:0.4695727233491584"
      ],
      "metadata": {
        "id": "PiRkLaYr4gLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline 5000 log\n",
        "\n",
        "Start epoch  95\n",
        "100%|██████████| 1692/1692 [04:53<00:00,  5.77it/s]\n",
        "[95,  1692] loss: 0.014397<br>\n",
        "Accuracy:0.7945894005469731\n",
        "Start epoch  96\n",
        "100%|██████████| 1692/1692 [04:53<00:00,  5.76it/s]\n",
        "[96,  1692] loss: 0.017905<br>\n",
        "Accuracy:0.7905240594278956\n",
        "Start epoch  97\n",
        "100%|██████████| 1692/1692 [04:53<00:00,  5.76it/s]\n",
        "[97,  1692] loss: 0.014716<br>\n",
        "Accuracy:0.7933328405647129\n",
        "Start epoch  98\n",
        "100%|██████████| 1692/1692 [04:54<00:00,  5.75it/s]\n",
        "[98,  1692] loss: 0.027101<br>\n",
        "Accuracy:0.7730800502623993\n",
        "Start epoch  99\n",
        "100%|██████████| 1692/1692 [04:52<00:00,  5.78it/s]\n",
        "[99,  1692] loss: 0.027469<br>\n",
        "Accuracy:0.7812107325005544\n",
        "Start epoch  100\n",
        "100%|██████████| 1692/1692 [04:53<00:00,  5.76it/s][100,  1692] loss: 0.014166\n",
        "Accuracy:0.7976938428560869<br>\n",
        "Finished Training<br>\n",
        "\n",
        "100%|██████████| 2317/2317 [01:25<00:00, 27.02it/s]Accuracy:0.4695727233491584"
      ],
      "metadata": {
        "id": "NFceQupvq9o-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CL 5000 log\n",
        "Start epoch  95\n",
        "100%|██████████| 1692/1692 [04:48<00:00,  5.86it/s]\n",
        "[95,  1692] loss: 0.018977\n",
        "Accuracy:0.7866804641880405 <br>\n",
        "Start epoch  96\n",
        "100%|██████████| 1692/1692 [04:48<00:00,  5.86it/s]\n",
        "[96,  1692] loss: 0.020678\n",
        "Accuracy:0.7894892453248578<br>\n",
        "Start epoch  97\n",
        "100%|██████████| 1692/1692 [04:48<00:00,  5.87it/s]\n",
        "[97,  1692] loss: 0.019395\n",
        "Accuracy:0.7870500406534112<br>\n",
        "Start epoch  98\n",
        "100%|██████████| 1692/1692 [04:47<00:00,  5.89it/s]\n",
        "[98,  1692] loss: 0.012431\n",
        "Accuracy:0.7951068075984922<br>\n",
        "Start epoch  99\n",
        "100%|██████████| 1692/1692 [04:48<00:00,  5.87it/s]\n",
        "[99,  1692] loss: 0.022111\n",
        "Accuracy:0.7860152265503733<br>\n",
        "Start epoch  100\n",
        "100%|██████████| 1692/1692 [04:46<00:00,  5.90it/s][100,  1692] loss: 0.012022\n",
        "Accuracy:0.7986547416660507<br>\n",
        "Finished Training\n",
        "\n",
        "100%|██████████| 2317/2317 [01:25<00:00, 26.97it/s]Accuracy:0.47561501942166595"
      ],
      "metadata": {
        "id": "DOaA6-JdeyRh"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "fit_CL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}